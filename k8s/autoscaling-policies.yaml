---
# Horizontal Pod Autoscaler for Orchestration Engine
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: orchestration-engine-hpa
  namespace: ai-agents
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: orchestration-engine
  minReplicas: 3
  maxReplicas: 100
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 5
        periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 10
        periodSeconds: 30
      selectPolicy: Max

---
# Vertical Pod Autoscaler for resource optimization
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: orchestration-engine-vpa
  namespace: ai-agents
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: orchestration-engine
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: orchestration-engine
      minAllowed:
        cpu: 100m
        memory: 256Mi
      maxAllowed:
        cpu: 4000m
        memory: 8Gi
      controlledResources: ["cpu", "memory"]

---
# Pod Disruption Budget to ensure high availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: orchestration-engine-pdb
  namespace: ai-agents
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: orchestration-engine

---
# HPA for Agent Execution Pools
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: agent-pool-hpa
  namespace: ai-agents
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: agent-execution-pool
  minReplicas: 10
  maxReplicas: 500
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 75
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 85
  - type: External
    external:
      metric:
        name: redis_queue_depth
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 180
      policies:
      - type: Percent
        value: 25
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 200
        periodSeconds: 15

---
# Cluster Autoscaler Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-priority-expander
  namespace: kube-system
data:
  priorities: |-
    10:
      - .*-spot-.*
    50:
      - .*-standard-.*
    100:
      - .*-premium-.*

---
# Keda ScaledObject for event-driven autoscaling
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: agent-queue-scaler
  namespace: ai-agents
spec:
  scaleTargetRef:
    name: agent-execution-pool
  minReplicaCount: 10
  maxReplicaCount: 500
  pollingInterval: 10
  cooldownPeriod: 60
  triggers:
  - type: redis
    metadata:
      address: redis:6379
      listName: agent_tasks
      listLength: "50"
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: agent_queue_size
      threshold: "100"
      query: sum(redis_list_length{list="agent_tasks"})

---
# Priority Classes for workload prioritization
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: enterprise-workload
value: 1000000
globalDefault: false
description: "Priority class for enterprise tier customers"

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: business-workload
value: 100000
globalDefault: false
description: "Priority class for business tier customers"

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: standard-workload
value: 10000
globalDefault: true
description: "Priority class for standard tier customers"

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: free-workload
value: 1000
globalDefault: false
description: "Priority class for free tier customers"

---
# Network Policy for security isolation
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: agent-network-policy
  namespace: ai-agents
spec:
  podSelector:
    matchLabels:
      tier: agent-execution
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: orchestration-engine
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443  # HTTPS for LLM APIs

---
# Resource Quotas per tenant namespace
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-enterprise-quota
  namespace: tenant-enterprise
spec:
  hard:
    requests.cpu: "1000"
    requests.memory: 2Ti
    limits.cpu: "2000"
    limits.memory: 4Ti
    persistentvolumeclaims: "100"
    services.loadbalancers: "10"

---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-business-quota
  namespace: tenant-business
spec:
  hard:
    requests.cpu: "100"
    requests.memory: 200Gi
    limits.cpu: "200"
    limits.memory: 400Gi
    persistentvolumeclaims: "20"
    services.loadbalancers: "2"

---
# Limit Range to prevent resource abuse
apiVersion: v1
kind: LimitRange
metadata:
  name: agent-limit-range
  namespace: ai-agents
spec:
  limits:
  - max:
      cpu: "4"
      memory: 8Gi
    min:
      cpu: 100m
      memory: 128Mi
    default:
      cpu: 500m
      memory: 512Mi
    defaultRequest:
      cpu: 250m
      memory: 256Mi
    type: Container
